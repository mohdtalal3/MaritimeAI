{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"ais_training_data\"]\n",
    "collection = db[\"station\"]\n",
    "\n",
    "# Load and filter valid sequences\n",
    "data = list(collection.find({\n",
    "    \"Visited_Ports\": {\"$exists\": True, \"$ne\": []},\n",
    "    \"Ship_Type\": {\"$exists\": True},\n",
    "    \"First_Port\": {\"$exists\": True},\n",
    "    \"Last_Port\": {\"$exists\": True}\n",
    "}))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.dropna(subset=[\"Ship_Type\", \"First_Port\", \"Last_Port\", \"Visited_Ports\"])\n",
    "\n",
    "valid_ship_types = np.array([\n",
    "    'Sailing', 'Military', 'Tug', 'Fishing', 'Pilot',\n",
    "    'Other', 'Port tender', 'Cargo', 'Pleasure', 'Passenger',\n",
    "    'Reserved', 'Tanker', 'SAR', 'HSC', 'Dredging',\n",
    "    'Not party to conflict', 'Law enforcement', 'Towing', 'Diving',\n",
    "    'Anti-pollution', 'Medical', 'Spare 1', 'WIG', 'Towing long/wide',\n",
    "    'Spare 2'\n",
    "], dtype=object)\n",
    "\n",
    "df[\"Ship_Type\"] = df[\"Ship_Type\"].apply(\n",
    "    lambda x: np.random.choice(valid_ship_types) if x == \"Undefined\" else x\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create vocabulary\n",
    "all_ports = set(sum(df[\"Visited_Ports\"].tolist(), []))\n",
    "all_ports.update(df[\"First_Port\"].tolist())\n",
    "all_ports.update(df[\"Last_Port\"].tolist())\n",
    "\n",
    "port_encoder = LabelEncoder()\n",
    "df[\"Visited_Ports_Str\"] = df[\"Visited_Ports\"].apply(lambda x: ','.join(x))\n",
    "port_encoder.fit(list(all_ports))\n",
    "\n",
    "ship_encoder = LabelEncoder()\n",
    "df[\"Ship_Type\"] = ship_encoder.fit_transform(df[\"Ship_Type\"])\n",
    "\n",
    "# Encode port sequences\n",
    "df[\"Visited_Ports_Encoded\"] = df[\"Visited_Ports\"].apply(lambda ports: port_encoder.transform(ports).tolist())\n",
    "df[\"First_Port_Encoded\"] = port_encoder.transform(df[\"First_Port\"])\n",
    "df[\"Last_Port_Encoded\"] = port_encoder.transform(df[\"Last_Port\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class RouteDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = [\n",
    "            torch.tensor([ship, first, last], dtype=torch.long)\n",
    "            for ship, first, last in zip(df[\"Ship_Type\"], df[\"First_Port_Encoded\"], df[\"Last_Port_Encoded\"])\n",
    "        ]\n",
    "        self.targets = [torch.tensor(seq, dtype=torch.long) for seq in df[\"Visited_Ports_Encoded\"]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RouteLSTM(nn.Module):\n",
    "    def __init__(self, port_vocab_size, ship_vocab_size, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.port_emb = nn.Embedding(port_vocab_size, 32)\n",
    "        self.ship_emb = nn.Embedding(ship_vocab_size, 8)\n",
    "        self.lstm = nn.LSTM(input_size=72, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, port_vocab_size)\n",
    "\n",
    "    def forward(self, ship_type, first_port, last_port, target_len):\n",
    "        # Create embeddings\n",
    "        ship_embed = self.ship_emb(ship_type).unsqueeze(1).expand(-1, target_len, -1)  # [B, T, 8]\n",
    "        first_embed = self.port_emb(first_port).unsqueeze(1).expand(-1, target_len, -1)\n",
    "        last_embed = self.port_emb(last_port).unsqueeze(1).expand(-1, target_len, -1)\n",
    "\n",
    "        combined = torch.cat([first_embed, last_embed, ship_embed], dim=2)\n",
    "        out, _ = self.lstm(combined)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 4052.6907\n",
      "Epoch 2/100, Loss: 3773.9596\n",
      "Epoch 3/100, Loss: 3514.3987\n",
      "Epoch 4/100, Loss: 3272.6896\n",
      "Epoch 5/100, Loss: 3047.6044\n",
      "Epoch 6/100, Loss: 2837.9999\n",
      "Epoch 7/100, Loss: 2642.8113\n",
      "Epoch 8/100, Loss: 2461.0472\n",
      "Epoch 9/100, Loss: 2291.7842\n",
      "Epoch 10/100, Loss: 2134.1626\n",
      "Epoch 11/100, Loss: 1987.3817\n",
      "Epoch 12/100, Loss: 1850.6960\n",
      "Epoch 13/100, Loss: 1723.4110\n",
      "Epoch 14/100, Loss: 1604.8803\n",
      "Epoch 15/100, Loss: 1494.5018\n",
      "Epoch 16/100, Loss: 1391.7147\n",
      "Epoch 17/100, Loss: 1295.9970\n",
      "Epoch 18/100, Loss: 1206.8625\n",
      "Epoch 19/100, Loss: 1123.8583\n",
      "Epoch 20/100, Loss: 1046.5629\n",
      "Epoch 21/100, Loss: 974.5837\n",
      "Epoch 22/100, Loss: 907.5549\n",
      "Epoch 23/100, Loss: 845.1362\n",
      "Epoch 24/100, Loss: 787.0104\n",
      "Epoch 25/100, Loss: 732.8824\n",
      "Epoch 26/100, Loss: 682.4771\n",
      "Epoch 27/100, Loss: 635.5385\n",
      "Epoch 28/100, Loss: 591.8282\n",
      "Epoch 29/100, Loss: 551.1241\n",
      "Epoch 30/100, Loss: 513.2195\n",
      "Epoch 31/100, Loss: 477.9219\n",
      "Epoch 32/100, Loss: 445.0520\n",
      "Epoch 33/100, Loss: 414.4427\n",
      "Epoch 34/100, Loss: 385.9387\n",
      "Epoch 35/100, Loss: 359.3951\n",
      "Epoch 36/100, Loss: 334.6770\n",
      "Epoch 37/100, Loss: 311.6590\n",
      "Epoch 38/100, Loss: 290.2241\n",
      "Epoch 39/100, Loss: 270.2634\n",
      "Epoch 40/100, Loss: 251.6755\n",
      "Epoch 41/100, Loss: 234.3661\n",
      "Epoch 42/100, Loss: 218.2472\n",
      "Epoch 43/100, Loss: 203.2368\n",
      "Epoch 44/100, Loss: 189.2588\n",
      "Epoch 45/100, Loss: 176.2422\n",
      "Epoch 46/100, Loss: 164.1208\n",
      "Epoch 47/100, Loss: 152.8331\n",
      "Epoch 48/100, Loss: 142.3218\n",
      "Epoch 49/100, Loss: 132.5333\n",
      "Epoch 50/100, Loss: 123.4181\n",
      "Epoch 51/100, Loss: 114.9298\n",
      "Epoch 52/100, Loss: 107.0253\n",
      "Epoch 53/100, Loss: 99.6644\n",
      "Epoch 54/100, Loss: 92.8098\n",
      "Epoch 55/100, Loss: 86.4267\n",
      "Epoch 56/100, Loss: 80.4825\n",
      "Epoch 57/100, Loss: 74.9472\n",
      "Epoch 58/100, Loss: 69.7926\n",
      "Epoch 59/100, Loss: 64.9925\n",
      "Epoch 60/100, Loss: 60.5225\n",
      "Epoch 61/100, Loss: 56.3599\n",
      "Epoch 62/100, Loss: 52.4837\n",
      "Epoch 63/100, Loss: 48.8740\n",
      "Epoch 64/100, Loss: 45.5126\n",
      "Epoch 65/100, Loss: 42.3824\n",
      "Epoch 66/100, Loss: 39.4675\n",
      "Epoch 67/100, Loss: 36.7530\n",
      "Epoch 68/100, Loss: 34.2253\n",
      "Epoch 69/100, Loss: 31.8714\n",
      "Epoch 70/100, Loss: 29.6794\n",
      "Epoch 71/100, Loss: 27.6381\n",
      "Epoch 72/100, Loss: 25.7372\n",
      "Epoch 73/100, Loss: 23.9671\n",
      "Epoch 74/100, Loss: 22.3187\n",
      "Epoch 75/100, Loss: 20.7837\n",
      "Epoch 76/100, Loss: 19.3543\n",
      "Epoch 77/100, Loss: 18.0232\n",
      "Epoch 78/100, Loss: 16.7836\n",
      "Epoch 79/100, Loss: 15.6293\n",
      "Epoch 80/100, Loss: 14.5543\n",
      "Epoch 81/100, Loss: 13.5533\n",
      "Epoch 82/100, Loss: 12.6212\n",
      "Epoch 83/100, Loss: 11.7531\n",
      "Epoch 84/100, Loss: 10.9448\n",
      "Epoch 85/100, Loss: 10.1920\n",
      "Epoch 86/100, Loss: 9.4911\n",
      "Epoch 87/100, Loss: 8.8383\n",
      "Epoch 88/100, Loss: 8.2304\n",
      "Epoch 89/100, Loss: 7.6644\n",
      "Epoch 90/100, Loss: 7.1372\n",
      "Epoch 91/100, Loss: 6.6464\n",
      "Epoch 92/100, Loss: 6.1892\n",
      "Epoch 93/100, Loss: 5.7636\n",
      "Epoch 94/100, Loss: 5.3672\n",
      "Epoch 95/100, Loss: 4.9980\n",
      "Epoch 96/100, Loss: 4.6543\n",
      "Epoch 97/100, Loss: 4.3342\n",
      "Epoch 98/100, Loss: 4.0361\n",
      "Epoch 99/100, Loss: 3.7585\n",
      "Epoch 100/100, Loss: 3.5000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "dataset = RouteDataset(df)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda batch: (\n",
    "    torch.stack([b[0] for b in batch]),\n",
    "    pad_sequence([b[1] for b in batch], batch_first=True, padding_value=0)\n",
    "))\n",
    "\n",
    "model = RouteLSTM(port_vocab_size=len(port_encoder.classes_), ship_vocab_size=len(ship_encoder.classes_))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "EPOCHS = 100\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for features, targets in loader:\n",
    "        ship_type = features[:, 0]\n",
    "        first_port = features[:, 1]\n",
    "        last_port = features[:, 2]\n",
    "\n",
    "        target_len = targets.shape[1]\n",
    "        outputs = model(ship_type, first_port, last_port, target_len)\n",
    "\n",
    "        loss = criterion(outputs.view(-1, outputs.shape[2]), targets.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model state\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"port_encoder\": port_encoder,\n",
    "    \"ship_encoder\": ship_encoder\n",
    "}, \"best_route_lstm_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_route(model, ship_type_str, first_port_str, last_port_str, max_len=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ship_type = torch.tensor([ship_encoder.transform([ship_type_str])[0]])\n",
    "        first_port = torch.tensor([port_encoder.transform([first_port_str])[0]])\n",
    "        last_port = torch.tensor([port_encoder.transform([last_port_str])[0]])\n",
    "\n",
    "        output = model(ship_type, first_port, last_port, target_len=max_len)\n",
    "        pred_ids = torch.argmax(output, dim=2).squeeze(0).tolist()\n",
    "        return [port_encoder.inverse_transform([i])[0] for i in pred_ids if i < len(port_encoder.classes_)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(seq):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for port in seq:\n",
    "        if port not in seen:\n",
    "            seen.add(port)\n",
    "            result.append(port)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted route: ['Aalborg', 'Copenhagen', 'Grenaa', 'Samso Island', 'Kastrup', 'Kyndby']\n"
     ]
    }
   ],
   "source": [
    "route = predict_route(model, \"Cargo\", \"Aalborg\", \"Havdrup\")\n",
    "cleaned_route = process(route)\n",
    "print(\"Predicted route:\", cleaned_route)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
