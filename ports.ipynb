{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch specific mmsi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing - Fetch Specific MMSI\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/ais_training_data.ais_data\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/ais_training_data.specific_mmsi_data\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.mongodb.input.batchSize\", \"100000\") \\\n",
    "    .config(\"spark.mongodb.output.batchSize\", \"10000\") \\\n",
    "    .config(\"spark.mongodb.output.bulk.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from MongoDB\n",
    "df = spark.read.format(\"mongo\").option(\"bulkRead\", \"true\").load()\n",
    "\n",
    "# Filter for the specific MMSI\n",
    "specific_mmsi = \"265610860\"\n",
    "mmsi_df = df.filter(col(\"MMSI\") == specific_mmsi)\n",
    "\n",
    "# Persist the DataFrame in memory\n",
    "mmsi_df = mmsi_df.persist()\n",
    "\n",
    "# Save to MongoDB\n",
    "output_collection = f\"mmsi_{specific_mmsi}_data\"\n",
    "mmsi_df.write.format(\"mongo\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"database\", \"ais_training_data\") \\\n",
    "    .option(\"collection\", output_collection) \\\n",
    "    .save()\n",
    "\n",
    "print(f\"Data for MMSI {specific_mmsi} has been saved to MongoDB collection: {output_collection}\")\n",
    "\n",
    "# Unpersist the DataFrame to free up memory\n",
    "mmsi_df.unpersist()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict ports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/10 03:21:28 WARN Utils: Your hostname, talal resolves to a loopback address: 127.0.1.1; using 192.168.187.27 instead (on interface wlo1)\n",
      "24/10/10 03:21:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/talal/.ivy2/cache\n",
      "The jars for the packages stored in: /home/talal/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bde1ae09-1989-48b0-b14d-626124eee7eb;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/talal/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 129ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bde1ae09-1989-48b0-b14d-626124eee7eb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/3ms)\n",
      "24/10/10 03:21:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Port stop processing completed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType, ArrayType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from pyspark.sql.functions import col, expr\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing - Fetch Specific MMSI\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/ais_training_data.ais_data\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/ais_training_data.specific_mmsi_data\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.mongodb.input.batchSize\", \"100000\") \\\n",
    "    .config(\"spark.mongodb.output.batchSize\", \"10000\") \\\n",
    "    .config(\"spark.mongodb.output.bulk.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Function to calculate Haversine distance\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "# Read port data from CSV\n",
    "ports_df = spark.read.csv(\"ports_output.csv\", header=True, inferSchema=True)\n",
    "ports_df = ports_df.select(\"Port Name\", \"Latitude\", \"Longitude\")\n",
    "\n",
    "# Collect ports data to driver for broadcasting\n",
    "ports_data = ports_df.collect()\n",
    "\n",
    "# Broadcast ports data\n",
    "broadcast_ports = spark.sparkContext.broadcast(ports_data)\n",
    "\n",
    "# UDF to find nearest port\n",
    "@udf(StructType([\n",
    "    StructField(\"NearestPort\", StringType()),\n",
    "    StructField(\"DistanceToPort\", DoubleType())\n",
    "]))\n",
    "def find_nearest_port(lat, lon):\n",
    "    min_distance = float('inf')\n",
    "    nearest_port = None\n",
    "    for port in broadcast_ports.value:\n",
    "        distance = haversine_distance(lat, lon, port['Latitude'], port['Longitude'])\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest_port = port['Port Name']\n",
    "    return (nearest_port, min_distance)\n",
    "\n",
    "# Read data from MongoDB\n",
    "df = spark.read.format(\"mongo\").option(\"bulkRead\", \"true\").load()\n",
    "\n",
    "processed_df = df.withColumn(\"LastLocation\", expr(\"Locations[size(Locations) - 1]\"))  \\\n",
    "    .withColumn(\"NearestPortInfo\", find_nearest_port(col(\"LastLocation.Latitude\"), col(\"LastLocation.Longitude\"))) \\\n",
    "    .withColumn(\"NearestPort\", col(\"NearestPortInfo.NearestPort\")) \\\n",
    "    .withColumn(\"DistanceToPort\", col(\"NearestPortInfo.DistanceToPort\")) \\\n",
    "    .withColumn(\"IsPortStop\", when(col(\"DistanceToPort\") < 10, True).otherwise(False)) \\\n",
    "    .withColumn(\"Destination\", when(col(\"IsPortStop\"), col(\"NearestPort\")).otherwise(None)) \\\n",
    "    .drop(\"LastLocation\", \"NearestPortInfo\")\n",
    "# Write processed data back to MongoDB\n",
    "processed_df.write.format(\"mongo\").mode(\"overwrite\").option(\"bulkWrite\", \"true\").save()\n",
    "\n",
    "print(\"Port stop processing completed.\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/11 16:27:22 WARN Utils: Your hostname, talal resolves to a loopback address: 127.0.1.1; using 192.168.10.112 instead (on interface wlo1)\n",
      "24/10/11 16:27:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/talal/.ivy2/cache\n",
      "The jars for the packages stored in: /home/talal/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4b186e34-6f39-4e6e-bbe8-89c76b7d6df0;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/talal/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 143ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4b186e34-6f39-4e6e-bbe8-89c76b7d6df0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n",
      "24/10/11 16:27:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+-----+\n",
      "|      Date|Ship_Type|    Destination|Count|\n",
      "+----------+---------+---------------+-----+\n",
      "|2023-09-10|Undefined|        Hamburg|  250|\n",
      "|2023-09-07|Undefined|        Hamburg|  214|\n",
      "|2023-06-08|Undefined|      Amsterdam|  210|\n",
      "|2023-06-07|Undefined|        Hamburg|  204|\n",
      "|2023-06-09|Undefined|        Hamburg|  201|\n",
      "|2023-09-11|Undefined|        Hamburg|  194|\n",
      "|2023-07-09|Undefined|          Donso|  179|\n",
      "|2023-06-08|Undefined|      Dordrecht|  179|\n",
      "|2023-06-08|Undefined|Hook of Holland|  178|\n",
      "|2023-07-09|Undefined|          Laboe|  158|\n",
      "|2023-06-20|Undefined|        Hamburg|  156|\n",
      "|2023-06-17|Undefined|          Donso|  155|\n",
      "|2023-06-07|Undefined|      Dordrecht|  155|\n",
      "|2023-06-11|Undefined|          Donso|  154|\n",
      "|2023-06-10|Undefined|          Donso|  148|\n",
      "|2023-06-08|Undefined|      Rotterdam|  147|\n",
      "|2023-06-06|Undefined|        Hamburg|  147|\n",
      "|2023-06-05|Undefined|          Donso|  145|\n",
      "|2023-09-17|Undefined|          Laboe|  144|\n",
      "|2023-06-13|Undefined|        Hamburg|  144|\n",
      "+----------+---------+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/11 16:51:24 WARN IndexShuffleBlockResolver: Error deleting data /tmp/blockmgr-59437d75-345c-42d6-b1e2-88eb52f42e5e/12/shuffle_1_2579_0.data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing completed and results stored in MongoDB.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, lit, desc\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/ais_training_data.specific_mmsi_data\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/ais_training_data.ais_data_summary\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.mongodb.input.batchSize\", \"100000\") \\\n",
    "    .config(\"spark.mongodb.output.batchSize\", \"10000\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.mongodb.output.bulk.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from MongoDB\n",
    "df = spark.read.format(\"mongo\").option(\"bulkRead\", \"true\").load()\n",
    "\n",
    "# Filter out rows with undefined ship type and empty destination\n",
    "\n",
    "# Add IsPortStop filter\n",
    "port_stop_df = df.filter(col(\"IsPortStop\") == True).filter(col(\"Destination\").isNotNull())\n",
    "\n",
    "# Group by Date, Ship_Type, and Destination, count occurrences, and sort in descending order\n",
    "grouped_df = port_stop_df.groupBy(\"Date\", \"Ship_Type\", \"Destination\") \\\n",
    "    .agg(count(\"*\").alias(\"Count\")) \\\n",
    "    .orderBy(desc(\"Count\"))\n",
    "\n",
    "grouped_df.show()\n",
    "input()\n",
    "\n",
    "# Write the sorted result to a new MongoDB collection\n",
    "grouped_df.write.format(\"mongo\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"database\", \"ais_training_data\") \\\n",
    "    .option(\"collection\", \"ais_data_summary\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data processing completed and results stored in MongoDB.\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+----------+\n",
      "|YearMonth|Ship_Type  |Destination        |TotalCount|\n",
      "+---------+-----------+-------------------+----------+\n",
      "|2023-12  |Sailing    |Frederikshavn      |31        |\n",
      "|2023-12  |Tug        |Aalborg            |31        |\n",
      "|2023-12  |Port tender|Orehoved           |31        |\n",
      "|2023-12  |Fishing    |Hanstholm          |31        |\n",
      "|2023-12  |Undefined  |Rodbyhavn          |31        |\n",
      "|2023-12  |Sailing    |Brondby            |31        |\n",
      "|2023-12  |Cargo      |Frederiksberg      |31        |\n",
      "|2023-12  |Military   |Kyndby             |31        |\n",
      "|2023-12  |Undefined  |Laboe              |31        |\n",
      "|2023-12  |Pleasure   |Allinge            |31        |\n",
      "|2023-12  |Reserved   |Asnaesvaerkets Havn|31        |\n",
      "|2023-12  |Tug        |Rodbyhavn          |31        |\n",
      "|2023-12  |Military   |Nykobing           |31        |\n",
      "|2023-12  |Passenger  |Laesohavn          |31        |\n",
      "|2023-12  |Tug        |Masnedsund         |31        |\n",
      "|2023-12  |Undefined  |Holtenau           |31        |\n",
      "|2023-12  |Undefined  |Skagen             |31        |\n",
      "|2023-12  |Pleasure   |Skagen             |31        |\n",
      "|2023-12  |Sailing    |Graasten           |31        |\n",
      "|2023-12  |Undefined  |Fredericia         |31        |\n",
      "+---------+-----------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly summary processed and saved to MongoDB.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, month, year, concat, lpad, desc\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Summary Processing\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/ais_training_data.ais_data_summary\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/ais_training_data.ais_data_monthly_summary\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from MongoDB\n",
    "df = spark.read.format(\"mongo\").load()\n",
    "\n",
    "# Extract year and month from the Date field and create a YYYY-MM format\n",
    "df_with_month = df.withColumn(\"Year\", year(col(\"Date\"))) \\\n",
    "                  .withColumn(\"Month\", lpad(month(col(\"Date\")), 2, \"0\")) \\\n",
    "                  .withColumn(\"YearMonth\", concat(col(\"Year\"), lit(\"-\"), col(\"Month\")))\n",
    "\n",
    "# Group by YearMonth, Ship_Type, and Destination, sum the Count\n",
    "monthly_summary = df_with_month.groupBy(\"YearMonth\", \"Ship_Type\", \"Destination\") \\\n",
    "    .agg(count(\"*\").alias(\"TotalCount\")) \\\n",
    "    .orderBy(desc(\"YearMonth\"), desc(\"TotalCount\"))\n",
    "\n",
    "# Show the results\n",
    "monthly_summary.show(n=20, truncate=False)\n",
    "\n",
    "# Write the results to a new MongoDB collection\n",
    "monthly_summary.write.format(\"mongo\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"database\", \"ais_training_data\") \\\n",
    "    .option(\"collection\", \"ais_data_monthly_summary\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Monthly summary processed and saved to MongoDB.\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------+\n",
      "|UniqueMMSICount|TotalLengthOfUniqueMMSI|\n",
      "+---------------+-----------------------+\n",
      "|          53929|                 483865|\n",
      "+---------------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique MMSI: 53929\n",
      "Total length of unique MMSI: 483865\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import countDistinct, length, sum\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Unique MMSI Count and Length\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/ais_training_data.specific_mmsi_data\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from MongoDB\n",
    "df = spark.read.format(\"mongo\").load()\n",
    "\n",
    "# Count unique MMSI values and calculate total length of unique MMSI\n",
    "result = df.select(\"MMSI\").distinct() \\\n",
    "    .select(\n",
    "        countDistinct(\"MMSI\").alias(\"UniqueMMSICount\"),\n",
    "        sum(length(\"MMSI\").cast(\"long\")).alias(\"TotalLengthOfUniqueMMSI\")\n",
    "    )\n",
    "\n",
    "# Show the results\n",
    "result.show()\n",
    "\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
