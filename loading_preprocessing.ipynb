{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/09 20:53:13 WARN Utils: Your hostname, talal resolves to a loopback address: 127.0.1.1; using 192.168.10.112 instead (on interface wlo1)\n",
      "24/10/09 20:53:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/talal/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/talal/.ivy2/cache\n",
      "The jars for the packages stored in: /home/talal/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c64858f6-b94f-4c22-bf80-4f48a2010705;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 144ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c64858f6-b94f-4c22-bf80-4f48a2010705\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n",
      "24/10/09 20:53:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, to_timestamp, collect_set, struct\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "from pyspark.sql.functions import collect_set,collect_list, struct, first, when, coalesce,date_format\n",
    "from pyspark.sql.functions import to_date, col, first, coalesce, when, collect_set, struct\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, collect_list, struct, first, when, coalesce, date_format, to_date, array_max\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Initialize Spark session with more memory and executor settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\")\\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/ais_training_data.ais_data\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/ais_training_data.ais_data\") \\\n",
    "    .config(\"spark.mongodb.output.batchSize\", \"10000\") \\\n",
    "    .config(\"spark.mongodb.output.bulk.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ZIP file: aisdk-2023-07.zip\n",
      "Processing file: aisdk-2023-07-01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/09 20:53:46 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-01.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-02.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-02.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-03.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-03.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-04.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-04.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-05.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-05.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-06.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-06.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-07.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-07.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-08.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-08.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-09.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-09.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-10.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-11.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-12.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-13.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-14.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-15.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-16.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-17.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-18.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-19.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-20.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-21.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-21.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-22.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-22.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-23.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-23.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-24.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-24.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-25.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-25.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-26.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-26.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-27.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-27.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-28.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-28.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-29.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-29.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-30.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-30.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-07-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-07-31.csv\n",
      "removing temp file\n",
      "Finished processing aisdk-2023-07.zip\n",
      "-----------------------------------\n",
      "All ZIP files processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define schema for the CSV files\n",
    "schema = StructType([\n",
    "    StructField(\"# Timestamp\", StringType(), True),\n",
    "    StructField(\"Type of mobile\", StringType(), True),\n",
    "    StructField(\"MMSI\", StringType(), True),\n",
    "    StructField(\"Latitude\", DoubleType(), True),\n",
    "    StructField(\"Longitude\", DoubleType(), True),\n",
    "    StructField(\"Navigational status\", StringType(), True),\n",
    "    StructField(\"ROT\", StringType(), True),\n",
    "    StructField(\"SOG\", StringType(), True),\n",
    "    StructField(\"COG\", StringType(), True),\n",
    "    StructField(\"Heading\", StringType(), True),\n",
    "    StructField(\"IMO\", StringType(), True),\n",
    "    StructField(\"Callsign\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Ship type\", StringType(), True),\n",
    "    StructField(\"Cargo type\", StringType(), True),\n",
    "    StructField(\"Width\", StringType(), True),\n",
    "    StructField(\"Length\", StringType(), True),\n",
    "    StructField(\"Type of position fixing device\", StringType(), True),\n",
    "    StructField(\"Draught\", StringType(), True),\n",
    "    StructField(\"Destination\", StringType(), True),\n",
    "    StructField(\"ETA\", StringType(), True),\n",
    "    StructField(\"Data source type\", StringType(), True),\n",
    "    StructField(\"A\", StringType(), True),\n",
    "    StructField(\"B\", StringType(), True),\n",
    "    StructField(\"C\", StringType(), True),\n",
    "    StructField(\"D\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Function to calculate Haversine distance\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "# Register UDF for Haversine distance\n",
    "haversine_udf = udf(haversine_distance, DoubleType())\n",
    "\n",
    "# Read port data from CSV\n",
    "ports_df = spark.read.csv(\"ports_output.csv\", header=True, inferSchema=True)\n",
    "ports_df = ports_df.select(\"Port Name\", \"Latitude\", \"Longitude\")\n",
    "\n",
    "# Broadcast the ports dataframe\n",
    "ports_broadcast = spark.sparkContext.broadcast(ports_df.collect())\n",
    "\n",
    "# Function to find nearest port\n",
    "def find_nearest_port(lat, lon):\n",
    "    min_distance = float('inf')\n",
    "    nearest_port = None\n",
    "    for port in ports_broadcast.value:\n",
    "        distance = haversine_distance(lat, lon, port['Latitude'], port['Longitude'])\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest_port = port['Port Name']\n",
    "    return nearest_port\n",
    "\n",
    "# Register UDF for finding nearest port\n",
    "find_nearest_port_udf = udf(find_nearest_port, StringType())\n",
    "\n",
    "def get_meaningful_value(column_name):\n",
    "    return coalesce(\n",
    "        first(when((col(column_name) != \"Unknown value\") & \n",
    "                   (col(column_name) != \"Undefined\") & \n",
    "                   (col(column_name).isNotNull()), \n",
    "                   col(column_name))),\n",
    "        first(col(column_name))\n",
    "    )\n",
    "\n",
    "# Function to process each DataFrame\n",
    "def process_dataframe(df):\n",
    "    # Transform the data\n",
    "    transformed_df = df.select(\n",
    "        date_format(to_date(col(\"# Timestamp\"), \"dd/MM/yyyy\"), \"yyyy-MM-dd\").alias(\"Date\"),\n",
    "        col(\"MMSI\"),\n",
    "        col(\"Latitude\"),\n",
    "        col(\"Longitude\"),\n",
    "        col(\"Navigational status\").alias(\"Nav_Status\"),\n",
    "        col(\"Ship type\").alias(\"Ship_Type\"),\n",
    "        col(\"Destination\")\n",
    "    )\n",
    "\n",
    "    # Group by MMSI, collecting unique lat-long pairs and meaningful values of other fields\n",
    "    grouped_df = transformed_df.groupBy(\"MMSI\") \\\n",
    "        .agg(\n",
    "            collect_list(struct(\"Latitude\", \"Longitude\")).alias(\"Locations\"),\n",
    "            get_meaningful_value(\"Date\").alias(\"Date\"),\n",
    "            get_meaningful_value(\"Nav_Status\").alias(\"Nav_Status\"),\n",
    "            get_meaningful_value(\"Ship_Type\").alias(\"Ship_Type\"),\n",
    "            get_meaningful_value(\"Destination\").alias(\"Destination\")\n",
    "        )\n",
    "\n",
    "    # Process the dataframe to update destinations\n",
    "    processed_df = grouped_df.withColumn(\n",
    "        \"LastLocation\", \n",
    "        array_max(\"Locations\")\n",
    "    ).withColumn(\n",
    "        \"NearestPort\",\n",
    "        find_nearest_port_udf(col(\"LastLocation.Latitude\"), col(\"LastLocation.Longitude\"))\n",
    "    ).withColumn(\n",
    "        \"Destination\",\n",
    "        col(\"NearestPort\")  # Always use the NearestPort as the Destination\n",
    "    ).drop(\"LastLocation\", \"NearestPort\")\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "# Function to read CSV data from a ZIP file\n",
    "def read_csv_from_zip(zip_path):\n",
    "    start_processing = False\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n",
    "        for file_name in zip_file.namelist():\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "            #if file_name == \"aisdk-2023-06-10.csv\":\n",
    "            start_processing = True\n",
    "            if start_processing and file_name.endswith('.csv'):\n",
    "                with zip_file.open(file_name) as csv_file:\n",
    "                    # Create a temporary file\n",
    "                    with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as temp_file:\n",
    "                        temp_file.write(csv_file.read())\n",
    "                        temp_file_path = temp_file.name\n",
    "                    \n",
    "                    try:\n",
    "                        # Read CSV content using the temporary file path\n",
    "                        df = spark.read.option(\"header\", \"true\").schema(schema).csv(temp_file_path)\n",
    "                        \n",
    "                        # Process the DataFrame\n",
    "                        processed_df = process_dataframe(df)\n",
    "                        # Write to MongoDB\n",
    "                        processed_df.write.format(\"mongo\") \\\n",
    "                            .option(\"uri\", \"mongodb://localhost:27017/ais_training_data.ais_data\") \\\n",
    "                            .mode(\"append\") \\\n",
    "                            .option(\"bulkWrite\", \"true\") \\\n",
    "                            .save()\n",
    "                        # Clear cache to free up memory\n",
    "                        spark.catalog.clearCache()\n",
    "                        \n",
    "                        print(f\"Completed processing file: {file_name}\")\n",
    "                    finally:\n",
    "                        print(\"removing temp file\")\n",
    "                        os.unlink(temp_file_path)\n",
    "\n",
    "zip_file_paths = [\n",
    "    r\"/media/talal/125866715866540F/FYP/Data AIS/aisdk-2023-07.zip\"\n",
    "]\n",
    "\n",
    "# Process the ZIP files\n",
    "for zip_file_path in zip_file_paths:\n",
    "    print(f\"Processing ZIP file: {os.path.basename(zip_file_path)}\")\n",
    "    try:\n",
    "        read_csv_from_zip(zip_file_path)\n",
    "        print(f\"Finished processing {os.path.basename(zip_file_path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {os.path.basename(zip_file_path)}: {str(e)}\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "print(\"All ZIP files processed\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/11 15:48:17 WARN Utils: Your hostname, talal resolves to a loopback address: 127.0.1.1; using 192.168.10.112 instead (on interface wlo1)\n",
      "24/10/11 15:48:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/talal/.ivy2/cache\n",
      "The jars for the packages stored in: /home/talal/.ivy2/jars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/talal/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5a6c5e27-db25-428a-8ef7-026cbcebeaba;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 137ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5a6c5e27-db25-428a-8ef7-026cbcebeaba\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n",
      "24/10/11 15:48:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ZIP file: aisdk-2023-11.zip\n",
      "Processing file: aisdk-2023-11-01.csv\n",
      "Processing file: aisdk-2023-11-02.csv\n",
      "Processing file: aisdk-2023-11-03.csv\n",
      "Processing file: aisdk-2023-11-04.csv\n",
      "Processing file: aisdk-2023-11-05.csv\n",
      "Processing file: aisdk-2023-11-06.csv\n",
      "Processing file: aisdk-2023-11-07.csv\n",
      "Processing file: aisdk-2023-11-08.csv\n",
      "Processing file: aisdk-2023-11-09.csv\n",
      "Processing file: aisdk-2023-11-10.csv\n",
      "Processing file: aisdk-2023-11-11.csv\n",
      "Processing file: aisdk-2023-11-12.csv\n",
      "Processing file: aisdk-2023-11-13.csv\n",
      "Processing file: aisdk-2023-11-14.csv\n",
      "Processing file: aisdk-2023-11-15.csv\n",
      "Processing file: aisdk-2023-11-16.csv\n",
      "Processing file: aisdk-2023-11-17.csv\n",
      "Processing file: aisdk-2023-11-18.csv\n",
      "Processing file: aisdk-2023-11-19.csv\n",
      "Processing file: aisdk-2023-11-20.csv\n",
      "Processing file: aisdk-2023-11-21.csv\n",
      "Processing file: aisdk-2023-11-22.csv\n",
      "Processing file: aisdk-2023-11-23.csv\n",
      "Processing file: aisdk-2023-11-24.csv\n",
      "Processing file: aisdk-2023-11-25.csv\n",
      "Processing file: aisdk-2023-11-26.csv\n",
      "Processing file: aisdk-2023-11-27.csv\n",
      "Processing file: aisdk-2023-11-28.csv\n",
      "Processing file: aisdk-2023-11-29.csv\n",
      "Processing file: aisdk-2023-11-30.csv\n",
      "Finished processing aisdk-2023-11.zip\n",
      "-----------------------------------\n",
      "Processing ZIP file: aisdk-2023-12.zip\n",
      "Processing file: aisdk-2023-12-01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/11 15:48:35 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-01.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-02.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-02.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-03.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-03.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-04.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-04.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-05.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-05.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-06.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-06.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-07.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-07.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-08.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-08.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-09.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-09.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-10.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-11.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-12.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-13.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-14.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-15.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-16.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-17.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-18.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-19.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-20.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-21.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-21.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-22.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-22.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-23.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-23.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-24.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-24.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-25.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-25.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-26.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-26.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-27.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-27.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-28.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-28.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-29.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-29.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-30.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-30.csv\n",
      "removing temp file\n",
      "Processing file: aisdk-2023-12-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing file: aisdk-2023-12-31.csv\n",
      "removing temp file\n",
      "Finished processing aisdk-2023-12.zip\n",
      "-----------------------------------\n",
      "All ZIP files processed\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, to_timestamp, collect_set, struct, collect_list, first, when, coalesce, date_format, to_date, array_max, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, BooleanType\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Initialize Spark session with more memory and executor settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\")\\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/ais_training_data.specific_mmsi_data\") \\\n",
    "    .config(\"spark.mongodb.output.batchSize\", \"10000\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.mongodb.output.bulk.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the CSV files\n",
    "schema = StructType([\n",
    "    StructField(\"# Timestamp\", StringType(), True),\n",
    "    StructField(\"Type of mobile\", StringType(), True),\n",
    "    StructField(\"MMSI\", StringType(), True),\n",
    "    StructField(\"Latitude\", DoubleType(), True),\n",
    "    StructField(\"Longitude\", DoubleType(), True),\n",
    "    StructField(\"Navigational status\", StringType(), True),\n",
    "    StructField(\"ROT\", StringType(), True),\n",
    "    StructField(\"SOG\", StringType(), True),\n",
    "    StructField(\"COG\", StringType(), True),\n",
    "    StructField(\"Heading\", StringType(), True),\n",
    "    StructField(\"IMO\", StringType(), True),\n",
    "    StructField(\"Callsign\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Ship type\", StringType(), True),\n",
    "    StructField(\"Cargo type\", StringType(), True),\n",
    "    StructField(\"Width\", StringType(), True),\n",
    "    StructField(\"Length\", StringType(), True),\n",
    "    StructField(\"Type of position fixing device\", StringType(), True),\n",
    "    StructField(\"Draught\", StringType(), True),\n",
    "    StructField(\"Destination\", StringType(), True),\n",
    "    StructField(\"ETA\", StringType(), True),\n",
    "    StructField(\"Data source type\", StringType(), True),\n",
    "    StructField(\"A\", StringType(), True),\n",
    "    StructField(\"B\", StringType(), True),\n",
    "    StructField(\"C\", StringType(), True),\n",
    "    StructField(\"D\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Function to calculate Haversine distance\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "# Read port data from CSV\n",
    "ports_df = spark.read.csv(\"ports_output.csv\", header=True, inferSchema=True)\n",
    "ports_df = ports_df.select(\"Port Name\", \"Latitude\", \"Longitude\")\n",
    "\n",
    "# Broadcast the ports dataframe\n",
    "ports_broadcast = spark.sparkContext.broadcast(ports_df.collect())\n",
    "\n",
    "# UDF to find nearest port\n",
    "@udf(StructType([\n",
    "    StructField(\"NearestPort\", StringType()),\n",
    "    StructField(\"DistanceToPort\", DoubleType())\n",
    "]))\n",
    "def find_nearest_port(lat, lon):\n",
    "    min_distance = float('inf')\n",
    "    nearest_port = None\n",
    "    for port in ports_broadcast.value:\n",
    "        distance = haversine_distance(lat, lon, port['Latitude'], port['Longitude'])\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest_port = port['Port Name']\n",
    "    return (nearest_port, min_distance)\n",
    "\n",
    "def get_meaningful_value(column_name):\n",
    "    return coalesce(\n",
    "        first(when((col(column_name) != \"Unknown value\") & \n",
    "                   (col(column_name) != \"Undefined\") & \n",
    "                   (col(column_name).isNotNull()), \n",
    "                   col(column_name))),\n",
    "        first(col(column_name))\n",
    "    )\n",
    "\n",
    "# Function to process each DataFrame\n",
    "def process_dataframe(df):\n",
    "    # Transform the data\n",
    "    transformed_df = df.select(\n",
    "        date_format(to_date(col(\"# Timestamp\"), \"dd/MM/yyyy\"), \"yyyy-MM-dd\").alias(\"Date\"),\n",
    "        col(\"MMSI\"),\n",
    "        col(\"Latitude\"),\n",
    "        col(\"Longitude\"),\n",
    "        col(\"Navigational status\").alias(\"Nav_Status\"),\n",
    "        col(\"Ship type\").alias(\"Ship_Type\"),\n",
    "        col(\"Destination\")\n",
    "    )\n",
    "\n",
    "    # Group by MMSI, collecting unique lat-long pairs and meaningful values of other fields\n",
    "    grouped_df = transformed_df.groupBy(\"MMSI\") \\\n",
    "        .agg(\n",
    "            collect_list(struct(\"Latitude\", \"Longitude\")).alias(\"Locations\"),\n",
    "            get_meaningful_value(\"Date\").alias(\"Date\"),\n",
    "            get_meaningful_value(\"Nav_Status\").alias(\"Nav_Status\"),\n",
    "            get_meaningful_value(\"Ship_Type\").alias(\"Ship_Type\"),\n",
    "            get_meaningful_value(\"Destination\").alias(\"Destination\")\n",
    "        )\n",
    "    # Process the dataframe to update destinations\n",
    "    processed_df = grouped_df.withColumn(\n",
    "        \"LastLocation\", \n",
    "        expr(\"Locations[size(Locations) - 1]\")\n",
    "    ).withColumn(\n",
    "        \"NearestPortInfo\",\n",
    "        find_nearest_port(col(\"LastLocation.Latitude\"), col(\"LastLocation.Longitude\"))\n",
    "    ).withColumn(\n",
    "        \"NearestPort\", \n",
    "        col(\"NearestPortInfo.NearestPort\")\n",
    "    ).withColumn(\n",
    "        \"DistanceToPort\", \n",
    "        col(\"NearestPortInfo.DistanceToPort\")\n",
    "    ).withColumn(\n",
    "        \"IsPortStop\", \n",
    "        when(col(\"DistanceToPort\") < 10, True).otherwise(False)\n",
    "    ).withColumn(\n",
    "        \"Destination\",\n",
    "        when(col(\"IsPortStop\"), col(\"NearestPort\")).otherwise(None)\n",
    "    ).drop(\"LastLocation\", \"NearestPortInfo\")\n",
    "    return processed_df\n",
    "\n",
    "# Function to read CSV data from a ZIP file\n",
    "def read_csv_from_zip(zip_path):\n",
    "    start_processing = False\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n",
    "        for file_name in zip_file.namelist():\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "            if file_name==\"aisdk-2023-12-01.csv\":\n",
    "                start_processing = True\n",
    "            if start_processing and file_name.endswith('.csv'):\n",
    "                with zip_file.open(file_name) as csv_file:\n",
    "                    # Create a temporary file\n",
    "                    with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as temp_file:\n",
    "                        temp_file.write(csv_file.read())\n",
    "                        temp_file_path = temp_file.name\n",
    "                    \n",
    "                    try:\n",
    "                        # Read CSV content using the temporary file path\n",
    "                        df = spark.read.option(\"header\", \"true\").schema(schema).csv(temp_file_path)\n",
    "                        \n",
    "                        # Process the DataFrame\n",
    "                        processed_df = process_dataframe(df)\n",
    "                        processed_df.write.format(\"mongo\") \\\n",
    "                            .option(\"uri\", \"mongodb://localhost:27017/ais_training_data.specific_mmsi_data\") \\\n",
    "                            .mode(\"append\") \\\n",
    "                            .option(\"bulkWrite\", \"true\") \\\n",
    "                            .save()\n",
    "                        # Clear cache to free up memory\n",
    "                        spark.catalog.clearCache()\n",
    "                        \n",
    "                        print(f\"Completed processing file: {file_name}\")\n",
    "                    finally:\n",
    "                        print(\"removing temp file\")\n",
    "                        os.unlink(temp_file_path)\n",
    "\n",
    "zip_file_paths = [\n",
    "    r\"/media/talal/125866715866540F/FYP/Data AIS/aisdk-2023-11.zip\",\n",
    "        r\"/media/talal/125866715866540F/FYP/Data AIS/aisdk-2023-12.zip\"\n",
    "]\n",
    "\n",
    "# Process the ZIP files\n",
    "for zip_file_path in zip_file_paths:\n",
    "    print(f\"Processing ZIP file: {os.path.basename(zip_file_path)}\")\n",
    "    try:\n",
    "        read_csv_from_zip(zip_file_path)\n",
    "        print(f\"Finished processing {os.path.basename(zip_file_path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {os.path.basename(zip_file_path)}: {str(e)}\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "print(\"All ZIP files processed\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
