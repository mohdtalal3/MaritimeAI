{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  YearWeek  Ship_Type  Destination  TotalCount\n",
      "0  2023-43  Undefined      Rostock          26\n",
      "1  2023-43  Undefined     Sassnitz          24\n",
      "2  2023-43  Undefined      Esbjerg          24\n",
      "3  2023-43  Undefined       Skagen          22\n",
      "4  2023-43  Undefined  Gotthenburg          21\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# MongoDB connection details\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")  # MongoDB URI\n",
    "db = client[\"ais_training_data\"]  # Database name\n",
    "collection = db[\"ais_data_relative_week_summary_simple\"]  # Collection name\n",
    "\n",
    "# Fetch data from MongoDB\n",
    "data = list(collection.find())  # Get all documents from the collection\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.drop(columns=['_id'])\n",
    "# Get a list of ship types excluding 'Undefined'\n",
    "ship_types = df['Ship_Type'][df['Ship_Type'] != 'Undefined'].unique()\n",
    "\n",
    "df['Ship_Type'] = df['Ship_Type'].apply(lambda x: np.random.choice(ship_types) if x == 'Undefined' else x)\n",
    "df=df.groupby(['YearWeek', 'Ship_Type', 'Destination'], as_index=False)['TotalCount'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearWeek</th>\n",
       "      <th>Ship_Type</th>\n",
       "      <th>Destination</th>\n",
       "      <th>TotalCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-43</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Rostock</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-43</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Sassnitz</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-43</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Esbjerg</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-43</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Skagen</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-43</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Gotthenburg</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40987</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>Dredging</td>\n",
       "      <td>Kerteminde</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40988</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>Dredging</td>\n",
       "      <td>Klagshamn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40989</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>Reserved</td>\n",
       "      <td>Helsingborg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40990</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>Dredging</td>\n",
       "      <td>Skagen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40991</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>Cargo</td>\n",
       "      <td>Kiel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40992 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      YearWeek  Ship_Type  Destination  TotalCount\n",
       "0      2023-43  Undefined      Rostock          26\n",
       "1      2023-43  Undefined     Sassnitz          24\n",
       "2      2023-43  Undefined      Esbjerg          24\n",
       "3      2023-43  Undefined       Skagen          22\n",
       "4      2023-43  Undefined  Gotthenburg          21\n",
       "...        ...        ...          ...         ...\n",
       "40987  2023-01   Dredging   Kerteminde           1\n",
       "40988  2023-01   Dredging    Klagshamn           1\n",
       "40989  2023-01   Reserved  Helsingborg           1\n",
       "40990  2023-01   Dredging       Skagen           1\n",
       "40991  2023-01      Cargo         Kiel           1\n",
       "\n",
       "[40992 rows x 4 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 1.72\n",
      "Mean Squared Error (MSE): 10.99\n",
      "Root Mean Squared Error (RMSE): 3.31\n",
      "R^2 Score: 0.84\n",
      "Predicted TotalCount: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('shipping_data.csv')\n",
    "\n",
    "# Preprocess YearWeek\n",
    "data[['Year', 'Week']] = data['YearWeek'].str.split('-', expand=True)\n",
    "data['Year'] = data['Year'].astype(int)\n",
    "data['Week'] = data['Week'].astype(int)\n",
    "data = data.drop('YearWeek', axis=1)\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop('TotalCount', axis=1)\n",
    "y = data['TotalCount']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing\n",
    "categorical_features = ['Ship_Type', 'Destination']\n",
    "numerical_features = ['Year', 'Week']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "        ('num', 'passthrough', numerical_features)\n",
    "    ])\n",
    "\n",
    "# Pipeline with model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R^2 Score: {r2:.2f}\")\n",
    "\n",
    "# Predict on new data\n",
    "new_data = pd.DataFrame({\n",
    "    'YearWeek': ['2024-05'],\n",
    "    'Ship_Type': ['WIG'],\n",
    "    'Destination': ['Frederiksvark']\n",
    "})\n",
    "\n",
    "new_data[['Year', 'Week']] = new_data['YearWeek'].str.split('-', expand=True)\n",
    "new_data['Year'] = new_data['Year'].astype(int)\n",
    "new_data['Week'] = new_data['Week'].astype(int)\n",
    "new_data = new_data.drop('YearWeek', axis=1)\n",
    "\n",
    "predicted_count = model.predict(new_data)\n",
    "print(f\"Predicted TotalCount: {predicted_count[0]:.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frequency_of_deployment_regressor.joblib']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib \n",
    "model_filename = 'frequency_of_deployment_regressor.joblib'\n",
    "joblib.dump(model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted TotalCount: 6\n"
     ]
    }
   ],
   "source": [
    "# Predict on new data\n",
    "new_data = pd.DataFrame({\n",
    "    'YearWeek': ['2024-1'],\n",
    "    'Ship_Type': ['Anti-pollution'],\n",
    "    'Destination': ['Gedser']\n",
    "})\n",
    "\n",
    "new_data[['Year', 'Week']] = new_data['YearWeek'].str.split('-', expand=True)\n",
    "new_data['Year'] = new_data['Year'].astype(int)\n",
    "new_data['Week'] = new_data['Week'].astype(int)\n",
    "new_data = new_data.drop('YearWeek', axis=1)\n",
    "\n",
    "predicted_count = model.predict(new_data)\n",
    "print(f\"Predicted TotalCount: {predicted_count[0]:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Ship_Type  Predicted_TotalCount\n",
      "0                 Sailing                     2\n",
      "1               Undefined                     8\n",
      "2                Military                     6\n",
      "3                     Tug                     4\n",
      "4                 Fishing                     2\n",
      "5                   Pilot                     4\n",
      "6                   Other                     7\n",
      "7             Port tender                     4\n",
      "8                   Cargo                     4\n",
      "9                Pleasure                     4\n",
      "10              Passenger                     2\n",
      "11               Reserved                     5\n",
      "12                 Tanker                     2\n",
      "13                    SAR                     3\n",
      "14                    HSC                     4\n",
      "15               Dredging                     4\n",
      "16  Not party to conflict                     4\n",
      "17        Law enforcement                     2\n",
      "18                 Towing                     3\n",
      "19                 Diving                     4\n",
      "20         Anti-pollution                     4\n",
      "21                Medical                     5\n",
      "22                Spare 1                     5\n",
      "23                    WIG                     4\n",
      "24       Towing long/wide                     4\n",
      "25                Spare 2                     4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "model_filename=\"frequency_of_deployment_regressor.joblib\"\n",
    "model = joblib.load(model_filename)\n",
    "print(\"\\nModel loaded successfully.\")\n",
    "\n",
    "\n",
    "# Array of all ship types\n",
    "ship_types = np.array([\n",
    "    'Sailing', 'Undefined', 'Military', 'Tug', 'Fishing', 'Pilot',\n",
    "    'Other', 'Port tender', 'Cargo', 'Pleasure', 'Passenger',\n",
    "    'Reserved', 'Tanker', 'SAR', 'HSC', 'Dredging',\n",
    "    'Not party to conflict', 'Law enforcement', 'Towing', 'Diving',\n",
    "    'Anti-pollution', 'Medical', 'Spare 1', 'WIG', 'Towing long/wide',\n",
    "    'Spare 2'\n",
    "], dtype=object)\n",
    "\n",
    "# Function to generate predictions with ceiling applied\n",
    "def generate_predictions(yearweek, destination, model, ship_types):\n",
    "    \"\"\"\n",
    "    Generates predictions for all ship types based on the provided YearWeek and Destination,\n",
    "    applying the ceiling function to the predicted TotalCount.\n",
    "\n",
    "    Parameters:\n",
    "    - yearweek (str): The YearWeek in 'YYYY-WW' format, e.g., '2025-12'.\n",
    "    - destination (str): The destination location, e.g., 'Aabenraa'.\n",
    "    - model: The pre-trained prediction model.\n",
    "    - ship_types (array-like): Array of ship type strings.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing Ship_Type and their Predicted_TotalCount (ceiled).\n",
    "    \"\"\"\n",
    "    # Create dataframe with all ship types\n",
    "    new_data = pd.DataFrame({\n",
    "        'Yearweek': [yearweek] * len(ship_types),\n",
    "        'Ship_Type': ship_types,\n",
    "        'Destination': [destination] * len(ship_types)\n",
    "    })\n",
    "\n",
    "    # Split Yearweek into Year and Week\n",
    "    try:\n",
    "        new_data[['Year', 'Week']] = new_data['Yearweek'].str.split('-', expand=True)\n",
    "        new_data['Year'] = new_data['Year'].astype(int)\n",
    "        new_data['Week'] = new_data['Week'].astype(int)\n",
    "    except Exception as e:\n",
    "        print(\"Error processing Yearweek. Ensure it's in 'YYYY-WW' format.\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "    new_data = new_data.drop('Yearweek', axis=1)\n",
    "\n",
    "\n",
    "    try:\n",
    "        predicted_counts = model.predict(new_data)\n",
    "    except Exception as e:\n",
    "        print(\"Error during prediction. Check if the input features match the model's expected format.\")\n",
    "        raise e\n",
    "\n",
    "    predicted_counts_ceiled = np.ceil(predicted_counts).astype(int)\n",
    "\n",
    "    new_data['Predicted_TotalCount'] = predicted_counts_ceiled\n",
    "\n",
    "    result_df = new_data[['Ship_Type', 'Predicted_TotalCount']]\n",
    "\n",
    "    return result_df\n",
    "\n",
    "user_yearweek = '2025-12'      \n",
    "user_destination = 'Aabenraa'   \n",
    "\n",
    "\n",
    "predictions_df = generate_predictions(user_yearweek, user_destination, model, ship_types)\n",
    "\n",
    "print(predictions_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 32790\n",
      "Testing samples: 8198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/talal/fyp/venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_21\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_21\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_42 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_54 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_58 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_59 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">133,441</span> (521.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m133,441\u001b[0m (521.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">133,441</span> (521.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m133,441\u001b[0m (521.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m410/410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0017 - mae: 0.0220 - val_loss: 6.0666e-04 - val_mae: 0.0129\n",
      "Epoch 2/10\n",
      "\u001b[1m410/410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.3664e-04 - mae: 0.0134 - val_loss: 4.8542e-04 - val_mae: 0.0109\n",
      "Epoch 3/10\n",
      "\u001b[1m410/410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.2088e-04 - mae: 0.0117 - val_loss: 4.5878e-04 - val_mae: 0.0104\n",
      "Epoch 4/10\n",
      "\u001b[1m410/410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.2975e-04 - mae: 0.0107 - val_loss: 4.6151e-04 - val_mae: 0.0098\n",
      "Epoch 5/10\n",
      "\u001b[1m410/410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.6915e-04 - mae: 0.0102 - val_loss: 4.3854e-04 - val_mae: 0.0095\n",
      "Epoch 6/10\n",
      "\u001b[1m410/410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0182e-04 - mae: 0.0097 - val_loss: 4.6977e-04 - val_mae: 0.0102\n",
      "Epoch 7/10\n",
      "\u001b[1m410/410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.4298e-04 - mae: 0.0097 - val_loss: 4.7558e-04 - val_mae: 0.0100\n",
      "Epoch 8/10\n",
      "\u001b[1m410/410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2353e-04 - mae: 0.0096 - val_loss: 4.7484e-04 - val_mae: 0.0097\n",
      "Epoch 9/10\n",
      "\u001b[1m410/410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7660e-04 - mae: 0.0094 - val_loss: 4.4988e-04 - val_mae: 0.0092\n",
      "Epoch 10/10\n",
      "\u001b[1m410/410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2883e-04 - mae: 0.0092 - val_loss: 4.9471e-04 - val_mae: 0.0103\n",
      "Test MAE (scaled): 0.0111\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 1.77\n",
      "Mean Squared Error (MSE): 20.22\n",
      "Root Mean Squared Error (RMSE): 4.50\n",
      "R^2 Score: 0.69\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Predicted TotalCount: 6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# 1. Load Data\n",
    "data = df\n",
    "\n",
    "# 2. Convert YearWeek to Date\n",
    "def yearweek_to_datetime(yearweek_str):\n",
    "    year, week = map(int, yearweek_str.split('-'))\n",
    "    return pd.to_datetime(f'{year}-W{week}-1', format='%Y-W%W-%w')\n",
    "\n",
    "data['Date'] = data['YearWeek'].apply(yearweek_to_datetime)\n",
    "data = data.drop('YearWeek', axis=1)\n",
    "data = data.sort_values('Date')\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 3. Encode Categorical Variables\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # Updated parameter\n",
    "encoded_features = encoder.fit_transform(data[['Ship_Type', 'Destination']])\n",
    "encoded_feature_names = encoder.get_feature_names_out(['Ship_Type', 'Destination'])\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoded_feature_names)\n",
    "data = pd.concat([data.drop(['Ship_Type', 'Destination'], axis=1), encoded_df], axis=1)\n",
    "\n",
    "# 4. Prepare Features and Target\n",
    "window_size = 4\n",
    "feature_cols = data.columns.difference(['Date', 'TotalCount'])\n",
    "data_array = data[feature_cols].values  # Shape: (num_steps, features)\n",
    "target_array = data['TotalCount'].values  # Shape: (num_steps,)\n",
    "\n",
    "# 5. Scale Features\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the entire feature data\n",
    "scaled_data = feature_scaler.fit_transform(data_array)  # Shape: (num_steps, features)\n",
    "\n",
    "# Scale the target\n",
    "scaled_target = target_scaler.fit_transform(target_array.reshape(-1, 1))  # Shape: (num_steps, 1)\n",
    "\n",
    "# 6. Create Sequences\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(scaled_data) - window_size):\n",
    "    X.append(scaled_data[i:i + window_size])  # Each X is (window_size, features)\n",
    "    y.append(scaled_target[i + window_size])  # Each y is (1,)\n",
    "\n",
    "X = np.array(X)  # Shape: (num_samples, window_size, features)\n",
    "y = np.array(y)  # Shape: (num_samples, 1)\n",
    "\n",
    "# 7. Split Data\n",
    "split_index = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "print(f'Training samples: {X_train.shape[0]}')\n",
    "print(f'Testing samples: {X_test.shape[0]}')\n",
    "\n",
    "# 8. Build LSTM Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', input_shape=(window_size, X_train.shape[2]), return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.summary()\n",
    "\n",
    "# 9. Train the Model\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 10. Evaluate the Model\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test MAE (scaled): {test_mae:.4f}')\n",
    "\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test_original, y_pred)\n",
    "mse = mean_squared_error(y_test_original, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_original, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R^2 Score: {r2:.2f}\")\n",
    "\n",
    "# 11. Save the Encoder, Scalers, and Model for Future Use\n",
    "joblib.dump(encoder, 'onehot_encoder.joblib')\n",
    "joblib.dump(feature_scaler, 'feature_scaler.joblib')\n",
    "joblib.dump(target_scaler, 'target_scaler.joblib')\n",
    "model.save('lstm_model.h5')\n",
    "\n",
    "# 12. Predict on New Data\n",
    "def predict_new_total_count(new_yearweek, new_ship_type, new_destination, scaled_data, window_size=4):\n",
    "    \"\"\"\n",
    "    Predicts the TotalCount for a new data point based on the latest window_size -1 data points.\n",
    "\n",
    "    Parameters:\n",
    "    - new_yearweek (str): The YearWeek of the new data point (e.g., '2024-05').\n",
    "    - new_ship_type (str): The Ship_Type of the new data point (e.g., 'WIG').\n",
    "    - new_destination (str): The Destination of the new data point (e.g., 'Frederiksvark').\n",
    "    - scaled_data (np.ndarray): The entire scaled feature data (num_steps, features).\n",
    "    - window_size (int): The window size used for the LSTM model.\n",
    "\n",
    "    Returns:\n",
    "    - predicted_count (float): The predicted TotalCount.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame for the new data\n",
    "    new_data = pd.DataFrame({\n",
    "        'YearWeek': [new_yearweek],\n",
    "        'Ship_Type': [new_ship_type],\n",
    "        'Destination': [new_destination]\n",
    "    })\n",
    "\n",
    "    # Convert YearWeek to Date\n",
    "    new_data['Date'] = new_data['YearWeek'].apply(yearweek_to_datetime)\n",
    "    new_data = new_data.drop('YearWeek', axis=1)\n",
    "\n",
    "    # Encode Ship_Type and Destination using the previously fitted encoder\n",
    "    new_encoded = encoder.transform(new_data[['Ship_Type', 'Destination']])\n",
    "    new_encoded_df = pd.DataFrame(new_encoded, columns=encoded_feature_names)\n",
    "\n",
    "    # Concatenate with the new data\n",
    "    new_data = pd.concat([new_data.drop(['Ship_Type', 'Destination'], axis=1), new_encoded_df], axis=1)\n",
    "\n",
    "    # Select feature columns\n",
    "    new_features = new_data[feature_cols].values  # Shape: (1, features)\n",
    "\n",
    "    # Scale the new features\n",
    "    new_features_scaled = feature_scaler.transform(new_features)  # Shape: (1, features)\n",
    "\n",
    "    # Extract the last window_size -1 data points from scaled_data\n",
    "    if len(scaled_data) < (window_size - 1):\n",
    "        raise ValueError(f\"Not enough data to create a sequence. Need at least {window_size - 1} data points.\")\n",
    "\n",
    "    last_steps = scaled_data[-(window_size - 1):]  # Shape: (window_size -1, features)\n",
    "\n",
    "    # Concatenate the last_steps with new_features_scaled to form a new sequence\n",
    "    new_sequence_scaled = np.concatenate([last_steps, new_features_scaled], axis=0)  # Shape: (window_size, features)\n",
    "\n",
    "    # Reshape to 3D array for LSTM input: (1, window_size, features)\n",
    "    new_sequence_scaled = new_sequence_scaled.reshape(1, window_size, -1)\n",
    "\n",
    "    # Predict\n",
    "    predicted_scaled = model.predict(new_sequence_scaled)\n",
    "    predicted = target_scaler.inverse_transform(predicted_scaled)\n",
    "\n",
    "    return predicted[0][0]\n",
    "\n",
    "# Example Prediction\n",
    "try:\n",
    "    predicted_count = predict_new_total_count(\n",
    "        new_yearweek='2024-05',\n",
    "        new_ship_type='WIG',\n",
    "        new_destination='Frederiksvark',\n",
    "        scaled_data=scaled_data,\n",
    "        window_size=window_size\n",
    "    )\n",
    "    print(f\"Predicted TotalCount: {predicted_count:.0f}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during prediction: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Predicted TotalCount: 7\n"
     ]
    }
   ],
   "source": [
    "# 12. Predict on New Data\n",
    "def predict_new_total_count(new_yearweek, new_ship_type, new_destination, scaled_data, window_size=4):\n",
    "    \"\"\"\n",
    "    Predicts the TotalCount for a new data point based on the latest window_size -1 data points.\n",
    "\n",
    "    Parameters:\n",
    "    - new_yearweek (str): The YearWeek of the new data point (e.g., '2024-05').\n",
    "    - new_ship_type (str): The Ship_Type of the new data point (e.g., 'WIG').\n",
    "    - new_destination (str): The Destination of the new data point (e.g., 'Frederiksvark').\n",
    "    - scaled_data (np.ndarray): The entire scaled feature data (num_steps, features).\n",
    "    - window_size (int): The window size used for the LSTM model.\n",
    "\n",
    "    Returns:\n",
    "    - predicted_count (float): The predicted TotalCount.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame for the new data\n",
    "    new_data = pd.DataFrame({\n",
    "        'YearWeek': [new_yearweek],\n",
    "        'Ship_Type': [new_ship_type],\n",
    "        'Destination': [new_destination]\n",
    "    })\n",
    "\n",
    "    # Convert YearWeek to Date\n",
    "    new_data['Date'] = new_data['YearWeek'].apply(yearweek_to_datetime)\n",
    "    new_data = new_data.drop('YearWeek', axis=1)\n",
    "\n",
    "    # Encode Ship_Type and Destination using the previously fitted encoder\n",
    "    new_encoded = encoder.transform(new_data[['Ship_Type', 'Destination']])\n",
    "    new_encoded_df = pd.DataFrame(new_encoded, columns=encoded_feature_names)\n",
    "\n",
    "    # Concatenate with the new data\n",
    "    new_data = pd.concat([new_data.drop(['Ship_Type', 'Destination'], axis=1), new_encoded_df], axis=1)\n",
    "\n",
    "    # Select feature columns\n",
    "    new_features = new_data[feature_cols].values  # Shape: (1, features)\n",
    "\n",
    "    # Scale the new features\n",
    "    new_features_scaled = feature_scaler.transform(new_features)  # Shape: (1, features)\n",
    "\n",
    "    # Extract the last window_size -1 data points from scaled_data\n",
    "    if len(scaled_data) < (window_size - 1):\n",
    "        raise ValueError(f\"Not enough data to create a sequence. Need at least {window_size - 1} data points.\")\n",
    "\n",
    "    last_steps = scaled_data[-(window_size - 1):]  # Shape: (window_size -1, features)\n",
    "\n",
    "    # Concatenate the last_steps with new_features_scaled to form a new sequence\n",
    "    new_sequence_scaled = np.concatenate([last_steps, new_features_scaled], axis=0)  # Shape: (window_size, features)\n",
    "\n",
    "    # Reshape to 3D array for LSTM input: (1, window_size, features)\n",
    "    new_sequence_scaled = new_sequence_scaled.reshape(1, window_size, -1)\n",
    "\n",
    "    # Predict\n",
    "    predicted_scaled = model.predict(new_sequence_scaled)\n",
    "    predicted = target_scaler.inverse_transform(predicted_scaled)\n",
    "\n",
    "    return predicted[0][0]\n",
    "\n",
    "try:\n",
    "    predicted_count = predict_new_total_count(\n",
    "        new_yearweek='2024-43',\n",
    "        new_ship_type='Anti-pollution',\n",
    "        new_destination='Gedser',\n",
    "        scaled_data=scaled_data,\n",
    "        window_size=window_size\n",
    "    )\n",
    "    print(f\"Predicted TotalCount: {predicted_count:.0f}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during prediction: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
