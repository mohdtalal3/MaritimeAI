{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of Fleet_deployment from LLoyd dataset . Combining Ais data with LLoyd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/home/talal/fyp/predictive model/Lloyds/ship_data/tblShip.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = df[['LRIMOShipNo', 'ShipName', 'ShipStatus',\"MaritimeMobileServiceIdentityMMSINumber\",\"ShipManagerCompanyCode\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6293/4279810212.py:3: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2=pd.read_csv(\"/home/talal/fyp/predictive model/Lloyds/ship_data/tblCompanyFullDetailsAndParentCode.csv\",dtype={'OWCODE': str})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df2=pd.read_csv(\"/home/talal/fyp/predictive model/Lloyds/ship_data/tblCompanyFullDetailsAndParentCode.csv\",dtype={'OWCODE': str})\n",
    "df1=pd.read_csv(\"/home/talal/fyp/predictive model/Lloyds/ship_data/tblShip.csv\",dtype={'ShipManagerCompanyCode': str,\"MaritimeMobileServiceIdentityMMSINumber\":str})\n",
    "df1 = df1[df1['MaritimeMobileServiceIdentityMMSINumber'].notna()]\n",
    "df1 = df1[['LRIMOShipNo', 'ShipName', 'ShipStatus',\"MaritimeMobileServiceIdentityMMSINumber\",\"ShipManagerCompanyCode\"]]\n",
    "merged_df = pd.merge(df1, df2[['OWCODE', 'ShortCompanyName']], \n",
    "                     left_on='ShipManagerCompanyCode', \n",
    "                     right_on='OWCODE', \n",
    "                     how='left')\n",
    "\n",
    "df = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Ship_company_name.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/10 07:49:17 WARN Utils: Your hostname, talal resolves to a loopback address: 127.0.1.1; using 192.168.10.139 instead (on interface wlo1)\n",
      "24/12/10 07:49:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/talal/.ivy2/cache\n",
      "The jars for the packages stored in: /home/talal/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c924f53f-3d7d-4e45-be2d-e54887ac7a5a;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/talal/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 141ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c924f53f-3d7d-4e45-be2d-e54887ac7a5a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n",
      "24/12/10 07:49:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "ERROR:root:Exception while sending command.                    (92 + 12) / 3239]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/talal/fyp/venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=69>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/talal/fyp/venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/talal/fyp/venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/talal/fyp/venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/talal/fyp/venv/lib/python3.12/site-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/home/talal/fyp/venv/lib/python3.12/site-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/home/talal/fyp/venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/talal/fyp/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/talal/fyp/venv/lib/python3.12/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o28.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/talal/fyp/venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/talal/fyp/venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "[Stage 4:=>                                                    (94 + 12) / 3239]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing - MMSI Matching with CSV\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/ais_training_data.specific_mmsi_data\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/ais_training_data.new_specific_mmsi_data_cname\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.mongodb.output.bulk.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load CSV data into a Spark DataFrame\n",
    "ship_data_df = spark.read.csv(\"/home/talal/fyp/predictive model/Ship_company_name.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Selecting relevant columns from the CSV (MMSI and ShortCompanyName)\n",
    "ship_data_df = ship_data_df.select(\"MaritimeMobileServiceIdentityMMSINumber\", \"ShortCompanyName\")\n",
    "\n",
    "# Read MongoDB data\n",
    "\n",
    "skip_documents = 1568093  \n",
    "df = spark.read.format(\"mongo\").option(\"spark.mongodb.input.skip\", skip_documents) \\\n",
    "    .option(\"spark.mongodb.input.limit\", 196504).option(\"bulkRead\", \"true\").load()\n",
    "\n",
    "# Join the MongoDB data with the CSV data based on MMSI\n",
    "df_with_company_name = df.join(\n",
    "    ship_data_df,\n",
    "    df[\"MMSI\"] == ship_data_df[\"MaritimeMobileServiceIdentityMMSINumber\"],\n",
    "    how=\"left\"  # Use left join to keep all documents from MongoDB even if there's no match in the CSV\n",
    ")\n",
    "\n",
    "# Optionally, drop the unnecessary columns and rename columns as needed\n",
    "df_with_company_name = df_with_company_name.drop(\"MaritimeMobileServiceIdentityMMSINumber\") \\\n",
    "    .withColumnRenamed(\"ShortCompanyName\", \"CN\")\n",
    "\n",
    "# Create a new collection by saving the updated DataFrame back to MongoDB\n",
    "df_with_company_name.write.format(\"mongo\").mode(\"append\").option(\"bulkWrite\", \"true\").save()\n",
    "\n",
    "# Print completion message\n",
    "print(\"MMSI matching with CSV completed, and data saved in new collection.\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing - MMSI Matching with CSV\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/ais_training_data.ais_data\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/ais_training_data.specific_mmsi_data\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load CSV data into a Spark DataFrame\n",
    "ship_data_df = spark.read.csv(\"ship_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Selecting relevant columns from the CSV (MMSI and ShortCompanyName)\n",
    "ship_data_df = ship_data_df.select(\"MaritimeMobileServiceIdentityMMSINumber\", \"ShortCompanyName\")\n",
    "\n",
    "# Read MongoDB data\n",
    "df = spark.read.format(\"mongo\").option(\"bulkRead\", \"true\").load()\n",
    "\n",
    "# Join the MongoDB data with the CSV data based on MMSI\n",
    "df_with_company_name = df.join(\n",
    "    ship_data_df,\n",
    "    df[\"MMSI\"] == ship_data_df[\"MaritimeMobileServiceIdentityMMSINumber\"],\n",
    "    how=\"left\"  # Use left join to keep all documents from MongoDB even if there's no match in the CSV\n",
    ")\n",
    "\n",
    "\n",
    "df_with_company_name = df_with_company_name.drop(\"MaritimeMobileServiceIdentityMMSINumber\") \\\n",
    "    .withColumnRenamed(\"ShortCompanyName\", \"CompanyName\")\n",
    "\n",
    "# Create a new collection by saving the updated DataFrame back to MongoDB\n",
    "df_with_company_name.write.format(\"mongo\").mode(\"overwrite\").option(\"bulkWrite\", \"true\").save()\n",
    "\n",
    "# Print completion message\n",
    "print(\"MMSI matching with CSV completed, and data saved in new collection.\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/10 08:19:02 WARN Utils: Your hostname, talal resolves to a loopback address: 127.0.1.1; using 192.168.10.139 instead (on interface wlo1)\n",
      "24/12/10 08:19:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/talal/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/talal/.ivy2/cache\n",
      "The jars for the packages stored in: /home/talal/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f5dee3ae-2ae1-4e98-86bc-535f99f2243a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 139ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f5dee3ae-2ae1-4e98-86bc-535f99f2243a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n",
      "24/12/10 08:19:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing completed and results stored in MongoDB.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, lit, desc\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/ais_training_data.new_specific_mmsi_data_cname\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/ais_training_data.ais_data_summary\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.mongodb.input.batchSize\", \"100000\") \\\n",
    "    .config(\"spark.mongodb.output.batchSize\", \"10000\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.mongodb.output.bulk.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from MongoDB\n",
    "df = spark.read.format(\"mongo\").option(\"bulkRead\", \"true\").load()\n",
    "\n",
    "\n",
    "# Write the sorted result to a new MongoDB collection\n",
    "df.write.format(\"mongo\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"database\", \"ais_training_data\") \\\n",
    "    .option(\"collection\", \"ais_data_summary\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data processing completed and results stored in MongoDB.\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/10 11:07:24 WARN Utils: Your hostname, talal resolves to a loopback address: 127.0.1.1; using 192.168.10.139 instead (on interface wlo1)\n",
      "24/12/10 11:07:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/talal/.ivy2/cache\n",
      "The jars for the packages stored in: /home/talal/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2fc98cda-a88c-4803-96d7-fdb74ed094c7;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/talal/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 116ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2fc98cda-a88c-4803-96d7-fdb74ed094c7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/3ms)\n",
      "24/12/10 11:07:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing completed and results stored in MongoDB.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/ais_training_data.ais_data_summary\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/ais_training_data.ais_data_summary_new\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.mongodb.output.bulk.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from MongoDB\n",
    "df_mongo = spark.read.format(\"mongo\").option(\"bulkRead\", \"true\").load()\n",
    "\n",
    "# Load only \"Port Name\" and \"ShortCompanyName\" from CSV\n",
    "df_csv = spark.read.option(\"header\", \"true\").csv(\"/home/talal/fyp/predictive model/new_port.csv\")\n",
    "df_csv = df_csv.select(\"Port Name\", \"ShortCompanyName\")\n",
    "\n",
    "# Rename columns for consistency with MongoDB data\n",
    "df_csv = df_csv.withColumnRenamed(\"Port Name\", \"Destination\")\n",
    "\n",
    "# Perform a left join on the \"Destination\" field from MongoDB and \"Port Name\" from CSV\n",
    "df_joined = df_mongo.join(df_csv, on=\"Destination\", how=\"left\")\n",
    "\n",
    "# Update the \"cn\" field with \"ShortCompanyName\" if \"cn\" is null\n",
    "df_final = df_joined.withColumn(\n",
    "    \"CN\", \n",
    "    when(col(\"CN\").isNull(), col(\"ShortCompanyName\")).otherwise(col(\"CN\"))\n",
    ")\n",
    "\n",
    "\n",
    "df_final = df_final.drop(\"ShortCompanyName\")\n",
    "\n",
    "# Write the final result to MongoDB\n",
    "df_final.write.format(\"mongo\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"database\", \"ais_training_data\") \\\n",
    "    .option(\"collection\", \"ais_data_summary_new\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data processing completed and results stored in MongoDB.\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------------+------------------------------+----------+\n",
      "|YearWeek|Ship_Type|Destination        |CN                            |TotalCount|\n",
      "+--------+---------+-------------------+------------------------------+----------+\n",
      "|2023-52 |Cargo    |Esbjerg            |FOB SWATH AS                  |1         |\n",
      "|2023-52 |Sailing  |Bandholm           |NULL                          |1         |\n",
      "|2023-52 |Pilot    |Fredericia         |NULL                          |1         |\n",
      "|2023-52 |Pilot    |Skagen             |NULL                          |1         |\n",
      "|2023-52 |Military |Fredericia         |NULL                          |1         |\n",
      "|2023-52 |Tanker   |Asnaesvaerkets Havn|ERIK THUN AB                  |1         |\n",
      "|2023-52 |Undefined|Hvide Sande        |HOY P                         |1         |\n",
      "|2023-52 |Tug      |Ronne              |HEEREMA MARINE CONTRACTORS-NTH|1         |\n",
      "|2023-52 |Other    |Koge               |NULL                          |1         |\n",
      "|2023-52 |Undefined|Asnaesvaerkets Havn|NULL                          |1         |\n",
      "|2023-52 |Cargo    |Wallhamn           |NULL                          |1         |\n",
      "|2023-52 |Undefined|Wallhamn           |KOSTER MARIN AB               |1         |\n",
      "|2023-52 |Military |Faaborg            |NULL                          |1         |\n",
      "|2023-52 |Other    |Kobenhavn          |DENMARK GOVT SOFARTSSTYRELSEN |1         |\n",
      "|2023-52 |Cargo    |Kobenhavn          |FORESTWAVE NAVIGATION BV      |1         |\n",
      "|2023-52 |Passenger|Rudkobing          |LANGELANDS KOMMUNE            |1         |\n",
      "|2023-52 |Pilot    |Hirtshals          |NULL                          |1         |\n",
      "|2023-52 |Fishing  |Hanstholm          |KINGFISHER FISKERISELSKABET   |1         |\n",
      "|2023-52 |Cargo    |Klagshamn          |LANGH SHIP AB                 |1         |\n",
      "|2023-52 |Tanker   |Ensted             |NULL                          |1         |\n",
      "+--------+---------+-------------------+------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly summary processed and saved to MongoDB.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, weekofyear, year, concat, lit, lpad, desc\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Summary Processing\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/ais_training_data.ais_data_summary\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/ais_training_data.ais_data_weekly_summary\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from MongoDB\n",
    "df = spark.read.format(\"mongo\").load()\n",
    "\n",
    "df_with_week = df.withColumn(\"Year\", year(col(\"Date\"))) \\\n",
    "                 .withColumn(\"Week\", weekofyear(col(\"Date\"))) \\\n",
    "                 .withColumn(\"YearWeek\", concat(col(\"Year\"), lit(\"-\"), lpad(col(\"Week\"), 2, \"0\"))) \\\n",
    "\n",
    "# Group by YearWeek, Ship_Type, Destination, CN and sum the Count\n",
    "weekly_summary = df_with_week.groupBy(\"YearWeek\", \"Ship_Type\", \"Destination\") \\\n",
    "    .agg(count(\"*\").alias(\"TotalCount\")) \\\n",
    "    .orderBy(desc(\"YearWeek\"), desc(\"TotalCount\"))\n",
    "\n",
    "# Show the results\n",
    "weekly_summary.show(n=20, truncate=False)\n",
    "\n",
    "# Write the results to a new MongoDB collection\n",
    "weekly_summary.write.format(\"mongo\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"database\", \"ais_training_data\") \\\n",
    "    .option(\"collection\", \"ais_data_weekly_summary\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Weekly summary processed and saved to MongoDB.\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_237211/732292704.py:4: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  companies_df = pd.read_csv('/home/talal/fyp/predictive model/Lloyds/ship_data/tblCompanyFullDetailsAndParentCode.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two CSV files into pandas DataFrames\n",
    "companies_df = pd.read_csv('/home/talal/fyp/predictive model/Lloyds/ship_data/tblCompanyFullDetailsAndParentCode.csv')\n",
    "ports_df = pd.read_csv('/home/talal/fyp/predictive model/ports_output.csv')\n",
    "\n",
    "# Convert the country names to lowercase for case-insensitive matching\n",
    "companies_df['CountryName'] = companies_df['CountryName'].str.lower()\n",
    "ports_df['Country'] = ports_df['Country'].str.lower()\n",
    "\n",
    "# Merge the two DataFrames on the country name\n",
    "merged_df = pd.merge(companies_df, ports_df, left_on='CountryName', right_on='Country', how='left')\n",
    "\n",
    "final_df = merged_df[['ShortCompanyName', 'CountryName', 'Port Name']].dropna(subset=['Port Name'])\n",
    "\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "#final_df.to_csv('merged_companies_with_ports.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"new_port.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+----------+\n",
      "|YearWeek|Ship_Type|Destination|TotalCount|\n",
      "+--------+---------+-----------+----------+\n",
      "|2023-43 |Undefined|Rostock    |26        |\n",
      "|2023-43 |Undefined|Esbjerg    |24        |\n",
      "|2023-43 |Undefined|Sassnitz   |24        |\n",
      "|2023-43 |Undefined|Skagen     |22        |\n",
      "|2023-43 |Undefined|Gotthenburg|21        |\n",
      "|2023-43 |Undefined|Rendsburg  |20        |\n",
      "|2023-43 |Undefined|Holtenau   |19        |\n",
      "|2023-43 |Undefined|Brunsbuttel|18        |\n",
      "|2023-43 |Undefined|Visby      |17        |\n",
      "|2023-43 |Undefined|Rodbyhavn  |17        |\n",
      "|2023-43 |Undefined|Cuxhaven   |16        |\n",
      "|2023-43 |Undefined|Busum      |14        |\n",
      "|2023-43 |Undefined|Hvide Sande|13        |\n",
      "|2023-43 |Cargo    |Rostock    |13        |\n",
      "|2023-43 |Undefined|Grenaa     |13        |\n",
      "|2023-43 |Undefined|Kerteminde |11        |\n",
      "|2023-43 |Cargo    |Esbjerg    |11        |\n",
      "|2023-43 |Undefined|Butzfleth  |10        |\n",
      "|2023-43 |Undefined|Hirtshals  |10        |\n",
      "|2023-43 |Undefined|Gluckstadt |10        |\n",
      "+--------+---------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative week summary processed and saved to MongoDB.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, weekofyear, year, concat, lit, lpad, desc, datediff\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Summary Processing\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/ais_training_data.ais_data_summary_new\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/ais_training_data.ais_data_relative_week_summary_simple\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from MongoDB\n",
    "df = spark.read.format(\"mongo\").load()\n",
    "\n",
    "# Define the fixed first date (baseline date)\n",
    "first_date = \"2023-01-01\"\n",
    "\n",
    "# Add columns for Year, Week, and the relative week number\n",
    "df_with_week = df.withColumn(\"Year\", year(col(\"Date\"))) \\\n",
    "                 .withColumn(\"Week\", weekofyear(col(\"Date\"))) \\\n",
    "                 .withColumn(\"RelativeWeek\", (datediff(col(\"Date\"), lit(first_date)) / 7).cast(\"int\") + 1) \\\n",
    "                 .withColumn(\"YearWeek\", concat(col(\"Year\"), lit(\"-\"), lpad(col(\"RelativeWeek\"), 2, \"0\")))\n",
    "\n",
    "# Group by the relative week, Ship_Type, Destination, CN and sum the Count\n",
    "relative_week_summary = df_with_week.groupBy(\"YearWeek\", \"Ship_Type\", \"Destination\") \\\n",
    "    .agg(count(\"*\").alias(\"TotalCount\")) \\\n",
    "    .orderBy(desc(\"YearWeek\"), desc(\"TotalCount\"))\n",
    "\n",
    "# Show the results\n",
    "relative_week_summary.show(n=20, truncate=False)\n",
    "\n",
    "# Write the results to a new MongoDB collection\n",
    "relative_week_summary.write.format(\"mongo\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"database\", \"ais_training_data\") \\\n",
    "    .option(\"collection\", \"ais_data_relative_week_summary_simple\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Relative week summary processed and saved to MongoDB.\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
