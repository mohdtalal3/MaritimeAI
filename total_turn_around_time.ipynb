{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/21 18:19:44 WARN Utils: Your hostname, talal resolves to a loopback address: 127.0.1.1; using 192.168.0.119 instead (on interface wlo1)\n",
      "25/05/21 18:19:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/talal/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/talal/.ivy2/cache\n",
      "The jars for the packages stored in: /home/talal/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e7e1006d-57c3-4cac-b2ba-6f041639ed63;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 150ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e7e1006d-57c3-4cac-b2ba-6f041639ed63\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/6ms)\n",
      "25/05/21 18:19:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_date, when, lit, collect_set,\n",
    "    min, max, first, size, date_format\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import datediff\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing - Final Format\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/ais_training_data.new_specific_mmsi_data_cname\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/ais_training_data.station\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    # .config(\"spark.mongodb.input.pipeline\", \"[{ \\\"$limit\\\": 50000 }]\") \\\n",
    "    \n",
    "# Load limited data\n",
    "df = spark.read.format(\"mongo\").load()\n",
    "\n",
    "# Preprocessing\n",
    "df = df.withColumn(\"date\", to_date(\"Date\", \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"Nav_Status_LC\", col(\"Nav_Status\").cast(StringType())) \\\n",
    "       .withColumn(\"Is_At_Port\", when(\n",
    "           (col(\"IsPortStop\") == True) |\n",
    "           (col(\"Nav_Status_LC\").rlike(\"(?i)moored|at anchor|restricted manoeuvrability\")),\n",
    "           lit(1)\n",
    "       ).otherwise(0))\n",
    "\n",
    "# Persist to prevent re-computation\n",
    "#df = df.persist()\n",
    "\n",
    "# Window for getting last port\n",
    "w = Window.partitionBy(\"MMSI\").orderBy(col(\"date\").desc())\n",
    "\n",
    "# Last port using window\n",
    "last_port_df = df.filter(col(\"Is_At_Port\") == 1) \\\n",
    "    .withColumn(\"Last_Port\", first(\"NearestPort\", ignorenulls=True).over(w)) \\\n",
    "    .select(\"MMSI\", \"Last_Port\") \\\n",
    "    .dropna(subset=[\"Last_Port\"]).dropDuplicates([\"MMSI\"])\n",
    "\n",
    "# Base Aggregation\n",
    "agg_df = df.groupBy(\"MMSI\").agg(\n",
    "    min(\"date\").alias(\"First_Port_Date\"),\n",
    "    max(\"date\").alias(\"Last_Seen_Date\"),\n",
    "    collect_set(when(col(\"Is_At_Port\") == 1, col(\"NearestPort\"))).alias(\"Visited_Ports\"),\n",
    "    collect_set(when(col(\"Is_At_Port\") == 1, col(\"date\"))).alias(\"Stationing_Day_List\"),\n",
    "    first(when(col(\"Is_At_Port\") == 1, col(\"NearestPort\")), ignorenulls=True).alias(\"First_Port\"),\n",
    "    first(\"Ship_Type\", ignorenulls=True).alias(\"Ship_Type\")\n",
    ")\n",
    "\n",
    "agg_df = agg_df.withColumn(\"Stationing_Days\", size(col(\"Stationing_Day_List\")))\n",
    "\n",
    "# Join last port info\n",
    "agg_df = agg_df.join(last_port_df, on=\"MMSI\", how=\"left\")\n",
    "\n",
    "# Calculate overall turnaround time (first to last seen)\n",
    "\n",
    "agg_df = agg_df.withColumn(\n",
    "    \"Overall_Turnaround_Time_Days\",\n",
    "    datediff(\"Last_Seen_Date\", \"First_Port_Date\")\n",
    ")\n",
    "\n",
    "\n",
    "# Final formatting with date strings\n",
    "final_df = agg_df.select(\n",
    "    \"MMSI\",\n",
    "    date_format(\"First_Port_Date\", \"yyyy-MM-dd\").alias(\"First_Port_Date\"),\n",
    "    date_format(\"Last_Seen_Date\", \"yyyy-MM-dd\").alias(\"Last_Seen_Date\"),\n",
    "    \"Stationing_Days\",\n",
    "    \"Overall_Turnaround_Time_Days\",\n",
    "    \"First_Port\",\n",
    "    \"Last_Port\",\n",
    "    \"Visited_Ports\",\n",
    "    \"Ship_Type\" \n",
    ")\n",
    "\n",
    "# Save to MongoDB\n",
    "final_df.write.format(\"mongo\").mode(\"overwrite\").option(\"collection\", \"station\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_stationing_simple.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"ais_training_data\"]\n",
    "collection = db[\"station\"]\n",
    "\n",
    "# Load data\n",
    "data = list(collection.find({\n",
    "    \"Overall_Turnaround_Time_Days\": {\"$exists\": True},\n",
    "    \"Stationing_Days\": {\"$exists\": True}\n",
    "}))\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Clean and preprocess\n",
    "df[\"Stationing_Days\"] = df[\"Stationing_Days\"].apply(lambda x: 1 if x == 0 else x)\n",
    "\n",
    "ship_types = np.array([\n",
    "    'Sailing', 'Military', 'Tug', 'Fishing', 'Pilot',\n",
    "    'Other', 'Port tender', 'Cargo', 'Pleasure', 'Passenger',\n",
    "    'Reserved', 'Tanker', 'SAR', 'HSC', 'Dredging',\n",
    "    'Not party to conflict', 'Law enforcement', 'Towing', 'Diving',\n",
    "    'Anti-pollution', 'Medical', 'Spare 1', 'WIG', 'Towing long/wide',\n",
    "    'Spare 2'\n",
    "], dtype=object)\n",
    "\n",
    "df[\"Ship_Type\"] = df[\"Ship_Type\"].apply(lambda x: np.random.choice(ship_types) if x == \"Undefined\" else x)\n",
    "\n",
    "# Filter and drop missing values\n",
    "df_model = df[[\"Ship_Type\", \"First_Port\", \"Last_Port\", \"Overall_Turnaround_Time_Days\", \"Stationing_Days\"]].dropna()\n",
    "\n",
    "# Features and targets\n",
    "X = df_model[[\"Ship_Type\", \"First_Port\", \"Last_Port\"]]\n",
    "y1 = df_model[\"Overall_Turnaround_Time_Days\"]\n",
    "y2 = df_model[\"Stationing_Days\"]\n",
    "\n",
    "# Preprocessing for categorical features\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(handle_unknown='ignore'), [\"Ship_Type\", \"First_Port\", \"Last_Port\"])\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y1_train, y1_test = train_test_split(X, y1, test_size=0.2, random_state=42)\n",
    "_, _, y2_train, y2_test = train_test_split(X, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipelines\n",
    "model_turnaround = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestRegressor(random_state=42))\n",
    "])\n",
    "model_stationing = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Train models\n",
    "model_turnaround.fit(X_train, y1_train)\n",
    "model_stationing.fit(X_train, y2_train)\n",
    "\n",
    "# Evaluate\n",
    "pred_turn = model_turnaround.predict(X_test)\n",
    "pred_stat = model_stationing.predict(X_test)\n",
    "\n",
    "# Save models\n",
    "joblib.dump(model_turnaround, \"model_turnaround_simple.pkl\")\n",
    "joblib.dump(model_stationing, \"model_stationing_simple.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Turnaround Time Metrics:\n",
      "MAE:  2.35\n",
      "RMSE: 3.12\n",
      "R²:   0.92\n",
      "\n",
      "🔍 Stationing Days Metrics:\n",
      "MAE:  1.87\n",
      "RMSE: 2.65\n",
      "R²:   0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Turnaround Evaluation\n",
    "mae_turn = mean_absolute_error(y1_test, pred_turn)\n",
    "rmse_turn = np.sqrt(mean_squared_error(y1_test, pred_turn))\n",
    "r2_turn = r2_score(y1_test, pred_turn)\n",
    "\n",
    "# Stationing Evaluation\n",
    "mae_stat = mean_absolute_error(y2_test, pred_stat)\n",
    "rmse_stat = np.sqrt(mean_squared_error(y2_test, pred_stat))\n",
    "r2_stat = r2_score(y2_test, pred_stat)\n",
    "\n",
    "print(\"\\n🔍 Turnaround Time Metrics:\")\n",
    "print(f\"MAE:  {mae_turn:.2f}\")\n",
    "print(f\"RMSE: {rmse_turn:.2f}\")\n",
    "print(f\"R²:   {r2_turn:.2f}\")\n",
    "\n",
    "print(\"\\n🔍 Stationing Days Metrics:\")\n",
    "print(f\"MAE:  {mae_stat:.2f}\")\n",
    "print(f\"RMSE: {rmse_stat:.2f}\")\n",
    "print(f\"R²:   {r2_stat:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Predicted_Overall_Turnaround_Time_Days': 183.38, 'Predicted_Stationing_Days': 48}\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load models\n",
    "model_turnaround = joblib.load(\"model_turnaround_simple.pkl\")\n",
    "model_stationing = joblib.load(\"model_stationing_simple.pkl\")\n",
    "\n",
    "def predict_simple(ship_type, first_port, last_port):\n",
    "    input_df = pd.DataFrame([{\n",
    "        \"Ship_Type\": ship_type,\n",
    "        \"First_Port\": first_port,\n",
    "        \"Last_Port\": last_port\n",
    "    }])\n",
    "    pred_turnaround = model_turnaround.predict(input_df)[0]\n",
    "    pred_stationing = model_stationing.predict(input_df)[0]\n",
    "    return {\n",
    "        \"Predicted_Overall_Turnaround_Time_Days\": round(pred_turnaround, 2),\n",
    "        \"Predicted_Stationing_Days\": max(1, round(pred_stationing))\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "result = predict_simple(\"Cargo\", \"Aalborg\", \"Havdrup\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
